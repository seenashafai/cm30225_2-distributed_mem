{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red254\green230\blue187;\red252\green90\blue64;\red0\green0\blue0;
\red255\green255\blue188;}
{\*\expandedcolortbl;;\cssrgb\c100000\c91773\c78023;\cssrgb\c100000\c44604\c31621;\cssrgb\c0\c0\c0;
\cssrgb\c100000\c100000\c78387;}
\paperw11900\paperh16840\margl1440\margr1440\vieww13460\viewh14680\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 CM30225: Parallel Programming\
\
\

\f1\b0 \
Background:\
\
The 2nd coursework for CM30225: Parallel Programming looks to complete the task of matrix relaxation using parallel techniques in a distributed memory architecture. The distributed approach requires us to implement OpenMPI: a Message Passing Interface library which enables processors on separate chips to communicate with each other. This means we can tackle problems requiring heavy computational power by passing information between said processors. Each processor has its own memory, and transferring data can consume a lot of resources, and costs a lot of time. Therefore special consideration must be given to the allocation of data in memory, to reduce both the overheads of communication, and the cost of transferring the data itself.\
\
Problem Overview\
\
\'95 Relaxation\
The task at hand is that of matrix relaxation. Given a starting matrix, we calculate the average of each value\'92s neighbours, and replace the starting value with the average. An example is shown below:\
\
\cb2 Figure 1\
\cb1 \
If we consider matrix m, as shown in figure 1: the value at index [i,j], its neighbouring indices are: [i-1, j], [i+1, j], [i, j-1], [i, j+1]. Therefore the average of index: [i,j]\'92s neighbours is: \
\
\\eq avg = (m[i-1, j] + m[i+1, j] + m[i, j-1] + m[i, j+1])/4 //Sum neighbours & divide by 4\
m[i,j] = avg //Replace starting value with average\
\
This process is repeated across every cell of the matrix (excluding borders) until the values converge within a given tolerance. \
\
\'95Parallelisation\
We need to develop software which can calculate relaxation on all cells in a given matrix, across multiple processors.\
The concept is basic: each processor should be allocated a sub-division of the global matrix. With this allocation, it should relax its given section using a function similar to that described in the section above. Each processor can then return its updated section back into the matrix. A precision check can then be performed from the previous set of relaxations to the current set. If each cell is within the given tolerance to its previous value, the task is completed.\
\
A flow-chart depicting this process is shown below:\
\

\f0\b \cb3 //talk about flowchart\

\f1\b0 \cb1 Throughout the process of implementation, reduction of overheads should be our primary concern. \
\
\
Parallel Approach\
\
\'95 Work allocation\
In my previous coursework implementation for the shared memory architecture, I devised a mathematical formula to equally partition up the cells, which would work for any size matrix. This function ensured that each thread had the most even distribution of cells possible. However, based on feedback and my own reflection, this approach was over-engineered, and unnecessary for the task. In an ideal situation, the number of rows in the matrix (excluding borders) is divisible by the number of processors, so that each processor has the same number rows (and therefore cells) to relax. In these situations, work is allocated perfectly equally. However, in cases where this is not possible, we will be left with a remainder after division of rows and processors. \
\
\
Rather than distributing these remaining cells randomly across the processors, it would be simpler to just give these remaining cells to a new worker. \
\
By doing this, we remove the complex overhead \
\
\cf4 \cb5 EMAIL BRADFORD									\
\cf0 \cb1 \
\
\
\
\'95 Work distribution\
The MPI library accommodates for multiple ways of distributing values from a given dataset to other processors.\
We have already determined the allocation of the data, so now we need to communicate this to the processors so that they can perform the relaxation. One such way of communication is through the broadcast function: MPI_Bcast. \
\
This works as one might assume; it broadcasts a dataset from the root processor to all other processors. All recipients can then work on their own section of this dataset, and return the updated matrix. Then these matrices would be combined using the same index allocation by which they were partitioned originally. \
\
As always, we need to consider the consequences of this on the messaging overhead. With the broadcast approach, the same matrix would be distributed to every worker over the network, but each worker would only be computing a very small segment of the matrix. We can quickly see how this is not very efficient. Furthermore, this would scale extremely badly; when working with matrices of very large dimensions, huge amounts of network bandwidth would be taken up by sending the entire dataset to each processor. For these reasons, this is not the best approach.\
\
Another approach would be to give each processor a segment of the original dataset, only relevant to the data they are working on. Due to the nature of the relaxation function, the neighbours of each distributed cell would be included. Considering the ideal situation where each processor is allocated a fixed r number of rows, we would in-fact have to distribute n+2 rows to each worker, to include its neighbours above and below. Furthermore, if each row contains n computable cells (i.e. not including the border on each side), we would have to distribute (n+2) cells per row, to accommodate for neighbouring cells.\
This approach is implemented in MPI via the scatter function: MPI_Scattter.\
\
If we consider the consequential overhead of this Scatter approach, it is a lot less costly than the Broadcast approach; only the required segment of data is distributed, greatly reducing the network bandwidth required to communicate the data.\
\
\'95 Retrieval & Precision check\
\
Correctness Testing\
\
Scalability investigation.\
\
\
\
\
displacement = null\
send count = null\
}